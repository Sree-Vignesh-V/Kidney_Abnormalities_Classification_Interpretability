{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56693cdd-6708-4698-a893-b6bfbf432166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50,VGG16,VGG19\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG, load_img\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SeparableConv2D,BatchNormalization,GlobalAveragePooling2D,MaxPooling2D,concatenate, Conv2DTranspose,Conv2D\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ab64b7-73e3-4f79-8536-dca9288e1b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12446 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "img_size=224\n",
    "img_size_dim=[224,224]\n",
    "dimension=(img_size,img_size)\n",
    "zoom=[0.99,1.01]\n",
    "bright=[0.8,1.2]\n",
    "fill_mode='constant'\n",
    "data_format='channels_last'\n",
    "dir=r'D:\\Datasets\\CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone'\n",
    "data_gen=IDG(rescale=1./255,brightness_range=bright,zoom_range=zoom,data_format=data_format,fill_mode=fill_mode,horizontal_flip=True)\n",
    "train_data_gen=data_gen.flow_from_directory(directory=dir,target_size=dimension,batch_size=12446,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e0a1df-a947-4707-a425-05e2f3276546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.image.DirectoryIterator at 0x276adf6ee50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2fc5a7-a75f-4f0d-920c-70cfeb94f694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12446, 224, 224, 3) (12446, 4)\n"
     ]
    }
   ],
   "source": [
    "train_data_all,train_labels_all=train_data_gen.next()\n",
    "print(train_data_all.shape,train_labels_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60541606-ab15-42ce-8958-b41b04727aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12446, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "277f0ef0-2bb3-47cb-943e-e89bc5729f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3709\n",
      "5077\n",
      "1377\n",
      "2283\n",
      "12446\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "d=0\n",
    "e=0\n",
    "f=0\n",
    "for i in train_labels_all:\n",
    "    if i[0] == 1:\n",
    "        c=c+1\n",
    "    if i[1] == 1:\n",
    "        d=d+1\n",
    "    if i[2] == 1:\n",
    "        e=e+1\n",
    "    if i[3] == 1:\n",
    "        f=f+1\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(c+d+e+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e024dd-10f7-41ec-b7f4-2d0b7da0abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(train_data_all,train_labels_all,test_size=0.2,random_state=47)\n",
    "train_data,val_data,train_labels,val_labels = train_test_split(train_data_all,train_labels_all,test_size=0.2,random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a40593-6139-400c-b1f6-0f6c7814264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba709b90-bc7c-4a93-88e0-3c19d0153bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model= keras.applications.VGG19(include_top=False,input_shape=(224,224,3),pooling='max',classes=4,weights='imagenet')\n",
    "pretrained_model.trainable=False\n",
    "x=pretrained_model.output\n",
    "x=Flatten()(x)\n",
    "x=Dense(512,activation='relu')(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Dropout(0.5)(x)\n",
    "x=Dense(4,activation='softmax')(x)\n",
    "model=Model(inputs=pretrained_model.input,outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf4542-164e-4cff-871b-3db7a93f554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(0.0001),\n",
    "              loss=keras.losses.CategoricalCrossentropy(),metrics=[\"accuracy\"])\n",
    "model.fit(train_data,train_labels,epochs=10,\n",
    "                 validation_data=(val_data,val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e82354-5b68-43a0-96de-fd8838f34e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img(r\"D:\\Datasets\\CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\\Normal\\Normal- (29).jpg\")\n",
    "img=img.resize((224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7edf0-becb-4769-b895-5330e20a845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c17ef8-5ab8-4fee-abe6-252a81b29701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(model.predict(x),axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14c368-c556-40d6-a2a1-5df808f1201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, layer_name):\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n",
    "        \n",
    "    def generate(self, image_array, class_idx):\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = self.grad_model(image_array)\n",
    "            loss = predictions[:, class_idx]\n",
    "\n",
    "        output = conv_outputs[0]\n",
    "        grads = tape.gradient(loss, conv_outputs)[0]\n",
    "\n",
    "        guided_grads = tf.cast(output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n",
    "\n",
    "        weights = tf.reduce_mean(guided_grads, axis=(0, 1))\n",
    "\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, output), axis=-1)\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (image_array.shape[2], image_array.shape[1]))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ea75a-f5fa-4f3f-896e-156b35f25f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load and preprocess image\n",
    "img_path = \"D:/Datasets/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (4).jpg\"\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# Initialize GradCAM\n",
    "gradcam = GradCAM(test, 'block5_conv1')\n",
    "\n",
    "# Generate GradCAM heatmap\n",
    "class_idx = 0  # Specify the class index (0-999 for ImageNet)\n",
    "heatmap = gradcam.generate(x, class_idx)\n",
    "\n",
    "# Superimpose heatmap on the original image\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.4 + cv2.cvtColor(x[0], cv2.COLOR_RGB2BGR)\n",
    "plt.imshow(superimposed_img, cmap=\"jet\")\n",
    "plt.show()\n",
    "cv2.imwrite('D:/Datasets/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/gradcam_cyst.jpg', superimposed_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b98040-0608-4aa4-9fa1-f1e07d4dbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guided_relu(x):\n",
    "    def grad(dy):\n",
    "        return tf.cast(dy > 0, \"float32\") * tf.cast(x > 0, \"float32\") * dy\n",
    "    return tf.nn.relu(x), grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eaed20-c83b-41db-b613-4324fb5622a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in test.layers:\n",
    "    if isinstance(layer, tf.keras.layers.ReLU):\n",
    "        layer.activation = guided_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43afdec1-5d2c-46ed-b7a5-5e7bb5b2322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradcam(input_model, image, layer_name):\n",
    "    grad_model = tf.keras.models.Model([input_model.inputs], [input_model.get_layer(layer_name).output, input_model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, predictions = grad_model(image)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "    grads = tape.gradient(loss, conv_output)[0]\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_output = conv_output[0]\n",
    "    guided_grads = conv_output * pooled_grads[..., tf.newaxis]\n",
    "    return guided_grads, class_idx\n",
    "\n",
    "def guided_grad_cam(input_model, image, layer_name):\n",
    "    guided_grads, class_idx = get_gradcam(input_model, image, layer_name)\n",
    "    cam = tf.reduce_sum(guided_grads, axis=-1)\n",
    "    return cam, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85766b71-8081-4a8e-a8fc-569199e39ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(r\"D:\\Datasets\\CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\\Tumor\\Tumor- (334).jpg\")  # Load your example image here\n",
    "img=img.resize((224,224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4863eb-603b-43ad-906b-8a6ddefa58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam, class_idx = guided_grad_cam(test, x, \"block2_conv2\")\n",
    "print(class_idx)\n",
    "# Visualize the Guided Grad-CAM\n",
    "plt.imshow(cam, cmap=\"jet\")\n",
    "plt.show()\n",
    "#plt.savefig('D:/Datasets/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/guided_gradcam_cyst_2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58599a9d-6468-4343-a11f-339e25ecaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothgrad(input_model, image, n=50, stdev=0.1):\n",
    "    smoothgrads = []\n",
    "    for _ in range(n):\n",
    "        noise = np.random.normal(0, stdev, image.shape)\n",
    "        noisy_image = image + noise\n",
    "        gradients = tf.GradientTape(input_model.output, input_model.input, noisy_image)\n",
    "        smoothgrads.append(gradients)\n",
    "    smoothgrads = np.mean(smoothgrads, axis=0)\n",
    "    return smoothgrads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa678c-27cf-464c-8c7e-4f659c9f0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(r\"D:\\Datasets\\CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\\Tumor\\Tumor- (334).jpg\")  # Load your example image here\n",
    "img=img.resize((224,224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c6149-32df-4386-818d-a1b54bfa2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occlusion_map(model, image, patch_size, stride):\n",
    "    occlusion_map = np.zeros((image.shape[1], image.shape[2]))\n",
    "\n",
    "    for i in range(0, image.shape[1] - patch_size, stride):\n",
    "        for j in range(0, image.shape[2] - patch_size, stride):\n",
    "            input_image = image.copy()\n",
    "            input_image[:, i:i+patch_size, j:j+patch_size, :] = 0  # Occlude the patch\n",
    "\n",
    "            prediction = model.predict(input_image)\n",
    "            prob = tf.nn.softmax(prediction, axis=-1)\n",
    "            top_prob = np.max(prob[0])\n",
    "\n",
    "            occlusion_map[i:i+patch_size, j:j+patch_size] = top_prob\n",
    "\n",
    "    return occlusion_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb872f-3389-4150-bd0e-90dd9a35cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 60  # Size of the occlusion patch\n",
    "stride = 20  # Stride for the occlusion patch\n",
    "occlusion_map = create_occlusion_map(test, x, patch_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc82f59-0c40-41ec-a31c-5433666aadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(occlusion_map, cmap='jet', alpha=0.5)\n",
    "plt.imshow(img, alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8647667-9f96-452a-ae78-a75f46783308",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16=tf.keras.models.load_model(\"D:/Project/DL_project/vgg.keras\")\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfeb83a-d7bd-4601-b38d-3d51dcf1b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc623cde-527a-4eac-bde1-de05daa2df4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
